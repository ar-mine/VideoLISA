{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Process Dataset",
   "id": "923567713f850f8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T02:26:08.996664Z",
     "start_time": "2025-02-25T02:26:08.991365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%env HF_HOME=/media/automan/ExSpace/PreData\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# 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"
   ],
   "id": "944c164b7897dece",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/media/automan/ExSpace/PreData\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T01:27:22.902015Z",
     "start_time": "2025-02-25T01:27:21.959800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "dataset = json.load(open(\"./stage2.json\"))\n",
    "print(len(dataset))\n",
    "print(dataset[0])\n",
    "\n",
    "''' Download Original dataset '''\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"OpenGVLab/InternVid\", 'InternVid-10M')\n",
    "# print(dataset.shape)\n",
    "# print(dataset['FLT'][0])"
   ],
   "id": "2ebcbef91b0a513a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146577\n",
      "{'id': 'Mq57LwS2UtI', 'conversations': [{'from': 'human', 'value': '<video>\\nCould you provide a summary of the incidents that occurred at various timestamps in the video?'}, {'from': 'gpt', 'value': 'A crowd of people watching an asian news broadcast from <s0> to <e0>. An image of the news, showing people at a church from <s1> to <e1>.'}], 'meta': {'split': [77.3, 85.0], 'duration': 7.7, 'token': {'<s0>': 0, '<e0>': 3.5, '<s1>': 3.5, '<e1>': 7.7}}, 'source': 'internvid'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Download Original dataset '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download Datasets",
   "id": "cc1a7f9dd66674b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "import subprocess\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "SAVE_PATH = \"/media/automan/6E94666294662CB1/A_Content/Youtube/videos\"\n",
    "PREFIX = 'https://www.youtube.com/watch?v='\n",
    "ydl_opts = {\n",
    "    'quiet': True,  # 启动安静模式。如果与——verbose一起使用，则将日志打印到stderr\n",
    "    # 'username': USER_NAME,\n",
    "    # 'password': PASSWORD,\n",
    "    # 'logger': MyLogger(),\n",
    "    # 'retries': 50\n",
    "    # 'postprocessors': [{\n",
    "    #     'key': 'FFmpegExtractAudio',\n",
    "    #     'preferredcodec': suffix,\n",
    "    #     'preferredquality': '192',\n",
    "    # }],\n",
    "}\n",
    "\n",
    "def type_download(refer, source_type, save_path):\n",
    "    print('refer:' + refer + ' ' + source_type + ' start download')\n",
    "    if source_type == 'video+audio':\n",
    "        ydl_opts['format'] = 'bestvideo+bestaudio'\n",
    "    else:\n",
    "        ydl_opts['format'] = 'best' + source_type\n",
    "    ydl_opts['outtmpl'] = save_path + os.path.sep + '{}.%(ext)s'.format(refer)\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            print('downloading...')\n",
    "            ydl.download([PREFIX + refer])\n",
    "    except yt_dlp.utils.DownloadError as e:\n",
    "        # 三种youtube视频源丢失的报错\n",
    "        if not 'unavailable' in str(e) and not 'Private' in str(e) and not 'terminated' in str(e):\n",
    "            if source_type == 'video+audio':\n",
    "                ret = subprocess.call(\n",
    "                    'yt-dlp --format bestvideo+bestaudio ' + PREFIX + refer + ' -o ' + save_path + ' -R 50')\n",
    "            else:\n",
    "                ret = subprocess.call('yt-dlp --format best' + source_type + PREFIX + refer + ' -o ' + save_path + ' -R 50')\n",
    "            if ret:\n",
    "                print(refer + ' ' + source_type + str(e))\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            print(refer + ' ' + source_type + str(e))\n",
    "            time.sleep(1)\n",
    "        return 1\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "        print(refer + ' ' + source_type + ' done')\n",
    "        return 0\n",
    "\n",
    "def trim_video(input_path, output_path, start_time, end_time):\n",
    "    \"\"\"\n",
    "    裁剪视频并保存\n",
    "    :param input_path: 输入视频文件路径\n",
    "    :param output_path: 输出裁剪后的视频文件路径\n",
    "    :param start_time: 裁剪开始时间（秒）\n",
    "    :param end_time: 裁剪结束时间（秒）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取视频文件\n",
    "        video = VideoFileClip(input_path)\n",
    "\n",
    "        # 确保裁剪时间合理\n",
    "        if start_time < 0 or end_time > video.duration or start_time >= end_time:\n",
    "            print(\"错误：裁剪时间超出范围或无效！\")\n",
    "            return\n",
    "\n",
    "        # 裁剪视频\n",
    "        trimmed_video = video.subclip(start_time, end_time)\n",
    "\n",
    "        # 保存裁剪后的视频\n",
    "        trimmed_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "        print(f\"视频已成功裁剪并保存到 {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "\n",
    "def process_label(label, skip_list):\n",
    "    # 如果 label 在跳过列表中，则直接返回\n",
    "    if label['id'] in skip_list:\n",
    "        return None, None, None\n",
    "    video_path = os.path.join(SAVE_PATH, label['id'])\n",
    "    if not os.path.exists(video_path):\n",
    "        os.makedirs(video_path)\n",
    "    fail = False\n",
    "    # 如果文件夹为空，则尝试下载视频\n",
    "    if not os.listdir(video_path):\n",
    "        if type_download(label['id'], 'video+audio', video_path):\n",
    "            fail = True  # 下载失败\n",
    "    # 检查下载后的视频文件（支持多种格式）\n",
    "    for ext in (\"mp4\", \"mkv\", \"webm\"):\n",
    "        original_video_path = os.path.join(video_path, '{}.{}'.format(label['id'], ext))\n",
    "        if os.path.exists(original_video_path):\n",
    "            break\n",
    "    clip_video_path = os.path.join(video_path, '{}_vtime.mp4'.format(label['id']))\n",
    "    # 如果剪辑视频不存在，则根据给定时间段进行裁剪\n",
    "    if not os.path.exists(clip_video_path):\n",
    "        start_sec = label['meta']['split'][0]\n",
    "        end_sec = label['meta']['split'][1]\n",
    "        trim_video(original_video_path, clip_video_path, start_sec, end_sec)\n",
    "    return clip_video_path, label, fail\n",
    "\n",
    "def download_dataset_parallel(max_data_number, skip_list, num_workers=2):\n",
    "    # 根据 max_data_number 选择要处理的标签\n",
    "    if max_data_number > 0:\n",
    "        max_labels = dataset[:max_data_number]\n",
    "    else:\n",
    "        max_labels = dataset\n",
    "\n",
    "    clip_video_paths = []\n",
    "    modify_labels = []\n",
    "    fail_list = []\n",
    "\n",
    "    # 使用 ThreadPoolExecutor 进行多线程处理\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_label, label, skip_list): label for label in max_labels}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            # 如果当前 label 被跳过，则 result 为 (None, None, None)\n",
    "            if result[0] is None:\n",
    "                continue\n",
    "            clip_path, label, failed = result\n",
    "            clip_video_paths.append(clip_path)\n",
    "            modify_labels.append(label)\n",
    "            if failed:\n",
    "                fail_list.append(label['id'])\n",
    "    print(\"下载失败的 id 列表：\", fail_list)\n",
    "    return modify_labels, clip_video_paths\n",
    "\n",
    "# Original Version\n",
    "def download_dataset(max_data_number, skip_list):\n",
    "    fail_list = []\n",
    "    if max_data_number > 0:\n",
    "        max_labels = dataset[:max_data_number]\n",
    "    else:\n",
    "        max_labels = dataset\n",
    "    clip_video_paths = []\n",
    "    modify_labels = []\n",
    "    for label in max_labels:\n",
    "        if label['id'] in skip_list:\n",
    "            continue\n",
    "        video_path = os.path.join(SAVE_PATH, label['id'])\n",
    "        if not os.path.exists(video_path):\n",
    "            os.makedirs(video_path)\n",
    "        if not os.listdir(video_path):\n",
    "            if type_download(label['id'], 'video+audio', video_path):\n",
    "                fail_list.append(label['id'])\n",
    "        for e in (\"mp4\", \"mkv\", \"webm\"):\n",
    "            original_video_path = os.path.join(video_path, '{}.{}'.format(label['id'], e))\n",
    "            if os.path.exists(original_video_path):\n",
    "                break\n",
    "        clip_video_path = os.path.join(video_path, '{}_vtime.mp4'.format(label['id']))\n",
    "        if not os.path.exists(clip_video_path):\n",
    "            start_sec = label['meta']['split'][0]\n",
    "            end_sec = label['meta']['split'][1]\n",
    "            trim_video(original_video_path,\n",
    "                       clip_video_path,\n",
    "                       start_sec,\n",
    "                       end_sec)\n",
    "        clip_video_paths.append(clip_video_path)\n",
    "        modify_labels.append(label)\n",
    "    print(fail_list)\n",
    "    return modify_labels, clip_video_paths\n"
   ],
   "id": "9178c307a039e795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "skip_list = [\n",
    "    \"oV2kjFLrXrw\", \"FwT3NUZ6vk8\", \"3G2X7a65rO8\", \"ZZBxws6iEFE\", \"Ssz7Zf8VQEs\", \"zc9UWW6cX8M\", \"CUoitqAxsOo\", \"Nogldhpq8BQ\", \"d4VQV0Hr8w8\", \"1RfQmpM4nkc\", \"p2tEpmT6fOA\", \"LPjYqoSkUi0\", \"iNRhe7lHlr4\", \"qHfWmdvEYB0\", \"R2Zri26D3Nw\", \"4ReLtzwQrY8\", \"i4uX2KlyGs8\", \"8rJJHvKNy3A\", \"JfWu_gqhp0I\", \"mE5QSZBepXg\", \"1buEFjm0X2I\", \"_8EMm_Wsywc\", \"lg6CcvPc8o4\", \"0jMp51s9CqY\", \"D2nYM3Q3WU4\", \"okmbII6BPNY\", \"UwIt_Ny63gI\", \"002CTRIvZOI\", \"WVNCfFJcQGs\", \"yD_aRo8Prdw\", \"5zSSMjYMYFM\", \"xrHMonjyNs0\", \"OEES9_UMOo4\", \"KzJbe547lZk\", \"-EBKPJH5pGI\", \"mG8Q9HrgYMs\", \"ayH1OUMnF5Q\"\n",
    "]\n",
    "# labels, clip_video_paths = download_dataset(max_data_number=1000, skip_list=skip_list)\n",
    "labels, clip_video_paths = download_dataset_parallel(max_data_number=1000, skip_list=skip_list)"
   ],
   "id": "2adf30f438504b86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save to sub-dataset",
   "id": "166c53a8f76c5a12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "json.dump(labels, open(\"index.json\", \"w\"))",
   "id": "a487ec6a6d085b6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.Load Model",
   "id": "9763a91c5ca4da09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T02:26:21.614388Z",
     "start_time": "2025-02-25T02:26:14.580779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, AutoTokenizer\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法"
   ],
   "id": "f5d6a3466dd7be03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb17942e80694dbaaa163a8af00e231d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.Generate Dataset",
   "id": "7726fbb1c31931f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from qwen_vl_utils import fetch_video\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "'''\n",
    "def process_func(sample):\n",
    "    sample_id = sample['id']\n",
    "    video_path = os.path.join(SAVE_PATH, sample_id)\n",
    "    cache_path = os.path.join(video_path, '{}_vtime.h5'.format(sample_id))\n",
    "    if os.path.exists(cache_path):\n",
    "        with h5py.File(cache_path, \"r\") as f:\n",
    "            input_ids = f['input_ids'][()]\n",
    "            attention_mask = f['attention_mask'][()]\n",
    "            labels = f['labels'][()]\n",
    "            pixel_values_videos = f['pixel_values_videos'][()]\n",
    "            video_grid_thw = f['video_grid_thw'][()]\n",
    "    else:\n",
    "        \"\"\" Fetch clip videos file \"\"\"\n",
    "        clip_video_path = os.path.join(video_path, '{}_vtime.mp4'.format(sample_id))\n",
    "        video_input, video_sample_fps = fetch_video({\"video\": clip_video_path},\n",
    "                                                    return_video_sample_fps=True)\n",
    "        # print(f\"Frames shape: {video_input.shape}, FPS: {video_sample_fps}\")\n",
    "\n",
    "        \"\"\" Convert timestamp (token) to frame number (ntoken)\"\"\"\n",
    "        sample['meta']['ntoken'] = {}\n",
    "        time_interval = (video_input.shape[0]-1) / sample['meta']['duration']\n",
    "        output_content = sample['conversations'][1]['value']\n",
    "        for k in sample['meta']['token'].keys():\n",
    "            if not sample['meta']['token'][k] is None:\n",
    "                sample['meta']['ntoken'][k] = int(sample['meta']['token'][k]*time_interval)\n",
    "                output_content.replace(k, str(sample['meta']['ntoken'][k]))\n",
    "\n",
    "        \"\"\" Fetch text prompt input and output\"\"\"\n",
    "        prompt = sample['conversations'][0]['value'][8:]\n",
    "        max_new_tokens = 2048\n",
    "        total_pixels = 20480 * 28 * 28\n",
    "        min_pixels = 16 * 28 * 28\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"video\": video_path, \"total_pixels\": total_pixels, \"min_pixels\": min_pixels},\n",
    "            ]\n",
    "             },\n",
    "        ]\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )  # 获取文本\n",
    "        image_inputs = []\n",
    "        video_inputs = [video_input]\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {key: value.tolist() for key, value in inputs.items() if isinstance(value, torch.Tensor)} #tensor -> list,为了方便拼接\n",
    "        instruction = inputs\n",
    "\n",
    "        response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n",
    "\n",
    "        input_ids = (\n",
    "                instruction[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "        )\n",
    "\n",
    "        attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n",
    "        labels = (\n",
    "                [-100] * len(instruction[\"input_ids\"][0])\n",
    "                + response[\"input_ids\"]\n",
    "                + [tokenizer.pad_token_id]\n",
    "        )\n",
    "        pixel_values_videos = inputs['pixel_values_videos']\n",
    "        video_grid_thw = inputs['video_grid_thw']\n",
    "\n",
    "        # if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        #     input_ids = input_ids[:MAX_LENGTH]\n",
    "        #     attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        #     labels = labels[:MAX_LENGTH]\n",
    "\n",
    "        # 存储到 HDF5 文件\n",
    "        with h5py.File(cache_path, \"w\") as f:\n",
    "            f.create_dataset(\"input_ids\", data=input_ids)\n",
    "            f.create_dataset(\"attention_mask\", data=attention_mask)\n",
    "            f.create_dataset(\"labels\", data=labels)\n",
    "            f.create_dataset(\"pixel_values_videos\", data=inputs['pixel_values_videos'])\n",
    "            f.create_dataset(\"video_grid_thw\", data=inputs['video_grid_thw'])\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    labels = torch.tensor(labels)\n",
    "    pixel_values_videos = torch.tensor(pixel_values_videos)\n",
    "    video_grid_thw = torch.tensor(video_grid_thw).squeeze(0)  #由（1,h,w)变换为（h,w）\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n",
    "            \"pixel_values_videos\": pixel_values_videos, \"video_grid_thw\": video_grid_thw}\n",
    "'''\n",
    "\n",
    "def process_func(sample):\n",
    "    sample_id = sample['id']\n",
    "    video_path = os.path.join(SAVE_PATH, sample_id)\n",
    "    cache_path = os.path.join(video_path, '{}_vtime.h5'.format(sample_id))\n",
    "\n",
    "    \"\"\" Fetch clip videos file \"\"\"\n",
    "    clip_video_path = os.path.join(video_path, '{}_vtime.mp4'.format(sample_id))\n",
    "    video_input, video_sample_fps = fetch_video({\"video\": clip_video_path,\n",
    "                                                 \"max_frames\": 15},\n",
    "                                                return_video_sample_fps=True)\n",
    "    # print(f\"Frames shape: {video_input.shape}, FPS: {video_sample_fps}\")\n",
    "\n",
    "    \"\"\" Convert timestamp (token) to frame number (ntoken)\"\"\"\n",
    "    sample['meta']['ntoken'] = {}\n",
    "    time_interval = (video_input.shape[0]-1) / sample['meta']['duration']\n",
    "    output_content = sample['conversations'][1]['value']\n",
    "    for k in sample['meta']['token'].keys():\n",
    "        if not sample['meta']['token'][k] is None:\n",
    "            sample['meta']['ntoken'][k] = int(sample['meta']['token'][k]*time_interval)\n",
    "            output_content.replace(k, str(sample['meta']['ntoken'][k]))\n",
    "\n",
    "    \"\"\" Fetch text prompt input and output\"\"\"\n",
    "    prompt = sample['conversations'][0]['value'][8:]\n",
    "    max_new_tokens = 2048\n",
    "    total_pixels = 20480 * 28 * 28\n",
    "    min_pixels = 16 * 28 * 28\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\"video\": video_path, \"total_pixels\": total_pixels, \"min_pixels\": min_pixels},\n",
    "        ]\n",
    "         },\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )  # 获取文本\n",
    "    image_inputs = []\n",
    "    video_inputs = [video_input]\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: value.tolist() for key, value in inputs.items() if isinstance(value, torch.Tensor)} #tensor -> list,为了方便拼接\n",
    "    instruction = inputs\n",
    "\n",
    "    response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n",
    "\n",
    "    input_ids = (\n",
    "            instruction[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    )\n",
    "\n",
    "    attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n",
    "    labels = (\n",
    "            [-100] * len(instruction[\"input_ids\"][0])\n",
    "            + response[\"input_ids\"]\n",
    "            + [tokenizer.pad_token_id]\n",
    "    )\n",
    "    pixel_values_videos = inputs['pixel_values_videos']\n",
    "    video_grid_thw = inputs['video_grid_thw']\n",
    "\n",
    "    # if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "    #     input_ids = input_ids[:MAX_LENGTH]\n",
    "    #     attention_mask = attention_mask[:MAX_LENGTH]\n",
    "    #     labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    #input_ids = torch.tensor(input_ids)\n",
    "    #attention_mask = torch.tensor(attention_mask)\n",
    "    #labels = torch.tensor(labels)\n",
    "    #pixel_values_videos = torch.tensor(pixel_values_videos)\n",
    "    #video_grid_thw = torch.tensor(video_grid_thw).squeeze(0)  #由（1,h,w)变换为（h,w）\n",
    "    video_grid_thw = np.squeeze(video_grid_thw, axis=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n",
    "            \"pixel_values_videos\": pixel_values_videos, \"video_grid_thw\": video_grid_thw}"
   ],
   "id": "d8ae180411fb9c13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "train_ds = load_dataset(\"json\", data_files=\"index.json\", split=\"train\", streaming=True)\n",
    "train_dataset = train_ds.map(process_func)"
   ],
   "id": "b50c37d5a1b16a2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## (Optional) LazyDataset",
   "id": "961fe20ea1861dd8"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from qwen_vl_utils import fetch_video\n",
    "\n",
    "class LazyVideoDataset(Dataset):\n",
    "    def __init__(self, samples, save_path, processor, tokenizer, max_new_tokens=2048):\n",
    "        \"\"\"\n",
    "        samples: 包含所有样本（例如：labels列表），每个样本为字典形式\n",
    "        save_path: 保存处理后数据的根目录\n",
    "        processor, tokenizer: 数据预处理和文本处理工具\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.save_path = save_path\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        sample_id = sample['id']\n",
    "        video_path = os.path.join(self.save_path, sample_id)\n",
    "        cache_path = os.path.join(video_path, '{}_vtime.h5'.format(sample_id))\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            with h5py.File(cache_path, \"r\") as f:\n",
    "                input_ids = f['input_ids'][()]\n",
    "                attention_mask = f['attention_mask'][()]\n",
    "                labels = f['labels'][()]\n",
    "                pixel_values_videos = f['pixel_values_videos'][()]\n",
    "                video_grid_thw = f['video_grid_thw'][()]\n",
    "        else:\n",
    "            # 加载视频并获取视频采样帧率\n",
    "            clip_video_path = os.path.join(video_path, '{}_vtime.mp4'.format(sample_id))\n",
    "            video_input, video_sample_fps = fetch_video({\"video\": clip_video_path},\n",
    "                                                        return_video_sample_fps=True)\n",
    "            # 将时间戳转化为帧编号\n",
    "            sample['meta']['ntoken'] = {}\n",
    "            time_interval = (video_input.shape[0]-1) / sample['meta']['duration']\n",
    "            output_content = sample['conversations'][1]['value']\n",
    "            for k in sample['meta']['token'].keys():\n",
    "                if sample['meta']['token'][k] is not None:\n",
    "                    ntoken = int(sample['meta']['token'][k]*time_interval)\n",
    "                    sample['meta']['ntoken'][k] = ntoken\n",
    "                    output_content = output_content.replace(k, str(ntoken))\n",
    "\n",
    "            # 处理文本 prompt 和视频数据\n",
    "            prompt = sample['conversations'][0]['value'][8:]\n",
    "            total_pixels = 20480 * 28 * 28\n",
    "            min_pixels = 16 * 28 * 28\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"video\": video_path, \"total_pixels\": total_pixels, \"min_pixels\": min_pixels},\n",
    "                ]}\n",
    "            ]\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs = []\n",
    "            video_inputs = [video_input]\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            # 将 tensor 转换为 list 以便后续拼接\n",
    "            inputs = {key: value.tolist() for key, value in inputs.items() if isinstance(value, torch.Tensor)}\n",
    "            instruction = inputs\n",
    "\n",
    "            response = self.tokenizer(f\"{output_content}\", add_special_tokens=False)\n",
    "            input_ids = (\n",
    "                    instruction[\"input_ids\"][0] + response[\"input_ids\"] + [self.tokenizer.pad_token_id]\n",
    "            )\n",
    "            attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n",
    "            labels = (\n",
    "                    [-100] * len(instruction[\"input_ids\"][0])\n",
    "                    + response[\"input_ids\"]\n",
    "                    + [self.tokenizer.pad_token_id]\n",
    "            )\n",
    "            pixel_values_videos = inputs['pixel_values_videos']\n",
    "            video_grid_thw = inputs['video_grid_thw']\n",
    "\n",
    "            # 存储到 HDF5 文件以便后续复用，减少重复计算\n",
    "            os.makedirs(video_path, exist_ok=True)\n",
    "            with h5py.File(cache_path, \"w\") as f:\n",
    "                f.create_dataset(\"input_ids\", data=input_ids)\n",
    "                f.create_dataset(\"attention_mask\", data=attention_mask)\n",
    "                f.create_dataset(\"labels\", data=labels)\n",
    "                f.create_dataset(\"pixel_values_videos\", data=pixel_values_videos)\n",
    "                f.create_dataset(\"video_grid_thw\", data=video_grid_thw)\n",
    "\n",
    "        # 转换为 torch.tensor，注意 video_grid_thw 可能需要 squeeze 处理\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        labels = torch.tensor(labels)\n",
    "        pixel_values_videos = torch.tensor(pixel_values_videos)\n",
    "        video_grid_thw = torch.tensor(video_grid_thw).squeeze(0)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n",
    "                \"pixel_values_videos\": pixel_values_videos, \"video_grid_thw\": video_grid_thw}\n",
    "\n",
    "# 使用示例：\n",
    "# 假设 labels 是一个包含所有样本字典的列表\n",
    "dataset = LazyVideoDataset(samples=labels, save_path=SAVE_PATH, processor=processor, tokenizer=tokenizer)\n",
    "# DataLoader 会按需调用 __getitem__，实现懒加载\n",
    "#data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4)\n"
   ],
   "id": "6e974e0c3d408372"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_list(labels)\n",
    "train_dataset = train_ds.map(process_func,\n",
    "                             batched=False,\n",
    "                             load_from_cache_file=True,  # 如果之前映射结果已缓存，则直接加载\n",
    "                             keep_in_memory=False        # 不将所有数据加载到内存中\n",
    "                            )\n",
    "train_dataset.set_format(\"torch\")"
   ],
   "id": "49ef93b3473f5a44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4.Train Model",
   "id": "8e188b36d6efc69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=64,  # Lora 秩\n",
    "    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.05,  # Dropout 比例\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 获取LoRA模型\n",
    "peft_model = get_peft_model(model, config)\n",
    "\n",
    "# 配置训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2_5-VL-3B\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=10,\n",
    "    max_steps=1800,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "SAVE_PATH = \"/media/automan/6E94666294662CB1/A_Content/Youtube/videos\"\n",
    "# 配置Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "# 开启模型训练\n",
    "trainer.train()"
   ],
   "id": "c26a82203a0b0629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5.Test Model",
   "id": "9112abed6087e826"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T02:27:47.386390Z",
     "start_time": "2025-02-25T02:27:47.383116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "\n",
    "def predict(messages, model):\n",
    "    # 准备推理\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "    fps_inputs = video_kwargs['fps']\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        fps=fps_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # 生成输出\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return output_text[0]"
   ],
   "id": "1f3a66f21682f498",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T02:27:52.045592Z",
     "start_time": "2025-02-25T02:27:48.477751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===测试模式===\n",
    "# 配置测试参数\n",
    "val_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=True,  # 测试模式\n",
    "    r=8,  # Lora 秩\n",
    "    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.05,  # Dropout 比例\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 获取测试模型\n",
    "val_peft_model = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2_5-VL-3B/checkpoint-500\", config=val_config)\n",
    "\n",
    "# 读取测试数据\n",
    "with open(\"data_vl_test.json\", \"r\") as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "prompt = \"Could you provide a summary of the incidents that occurred at various timestamps in the video?\"\n",
    "video_path = \"/media/automan/6E94666294662CB1/A_Content/Youtube/videos/_3jEXkguVKI/_3jEXkguVKI_vtime.mp4\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\"video\": video_path, \"total_pixels\": 20480 * 28 * 28, \"min_pixels\": 16 * 28 * 28},\n",
    "    ]\n",
    "     },\n",
    "]\n",
    "\n",
    "response = predict(messages, val_peft_model)\n",
    "messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n",
    "print(messages[-1])"
   ],
   "id": "fa8734e074e342f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using decord to read video.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'From <s0> to <e0>, a person is cutting a lobster with a pair of scissors. From <s1> to <e1>, a person is cutting chicken into pieces on a plate.'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Extended Charts",
   "id": "b752df83bfbf8c56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zip_files(file_paths, output_zip):\n",
    "    \"\"\"\n",
    "    将 file_paths 列表中的所有文件打包到 output_zip 指定的 zip 文件中\n",
    "\n",
    "    参数：\n",
    "    file_paths -- 要打包的文件路径列表\n",
    "    output_zip -- 生成的 zip 文件路径\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in file_paths:\n",
    "            if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "                # 使用 os.path.basename 将文件存储为压缩包内的文件名（不包含原始路径）\n",
    "                arcname = os.path.basename(file_path)\n",
    "                zipf.write(file_path, arcname=arcname)\n",
    "                print(f\"添加 {file_path} 到 {output_zip} 中\")\n",
    "            else:\n",
    "                print(f\"警告：{file_path} 不存在或不是文件\")\n",
    "\n",
    "# 输出 zip 文件名\n",
    "output_zip = \"videos.zip\"\n",
    "\n",
    "zip_files(clip_video_paths, output_zip)\n",
    "print(f\"所有文件已打包到 {output_zip}\")\n"
   ],
   "id": "56f93a755e59f8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d5ee6674951f98ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
